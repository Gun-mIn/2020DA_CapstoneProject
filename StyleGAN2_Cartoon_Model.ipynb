{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN2-Cartoon-Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gun-mIn/2020DA_CapstoneProject/blob/main/StyleGAN2_Cartoon_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEx9IOnF1hKO"
      },
      "source": [
        "#Training StyleGAN2 on Colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcDHP4h-11Ii"
      },
      "source": [
        "##Mounting Google Drive\n",
        "So I’m actually gonna install my entire repo directly into Google Drive. This will make the setup a little easier, but its a little strange I admit.\n",
        "\n",
        "First, connect your Drive to Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJazuNYurryY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c5d4587-31d7-4242-855f-eef490f8aee9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aadrbMyR2F00"
      },
      "source": [
        "##Install the repo\n",
        "**Only do this for the first time ever setting this up!**\n",
        "\n",
        "If this is your first time ever running this notebook, you’ll want to install my fork of StyleGAN2 to your Drive account. Make sure you have ample space on your Drive (I’d say at least 50GB). This will install the repo and add some dependecies (like transferring from FFHQ the first time).\n",
        "\n",
        "Every time after your first use of this notebook you’ll want to skip this cell and run the cell after this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoTNQ3Gyr0ih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d0875f-982c-4145-e596-5779587f2ccc"
      },
      "source": [
        "#SKIP this if you already have a stylegan2 folder in your google drive\n",
        "%cd /content/drive/MyDrive/new/2. Baby Face Modeling\n",
        "!git clone https://github.com/dvschultz/stylegan2\n",
        "%cd stylegan2\n",
        "!mkdir pkl\n",
        "%cd pkl\n",
        "!gdown --id 1JLqXE5bGZnlu2BkbLPD5_ZxoO3Nz-AvF #inception: https://drive.google.com/open?id=1JLqXE5bGZnlu2BkbLPD5_ZxoO3Nz-AvF\n",
        "%cd ../\n",
        "!mkdir results\n",
        "!mkdir results/00001-pretrained\n",
        "%cd results/00001-pretrained\n",
        "!gdown --id 1UlDmJVLLnBD9SnLSMXeiZRO6g-OMQCA_\n",
        "!mv stylegan2-ffhq-config-f.pkl network-snapshot-10000.pkl\n",
        "%cd ../../\n",
        "%mkdir datasets\n",
        "%mkdir dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/new/2. Baby Face Modeling\n",
            "Cloning into 'stylegan2'...\n",
            "remote: Enumerating objects: 552, done.\u001b[K\n",
            "remote: Total 552 (delta 0), reused 0 (delta 0), pack-reused 552\u001b[K\n",
            "Receiving objects: 100% (552/552), 22.47 MiB | 19.30 MiB/s, done.\n",
            "Resolving deltas: 100% (285/285), done.\n",
            "/content/drive/MyDrive/new/2. Baby Face Modeling/stylegan2\n",
            "/content/drive/MyDrive/new/2. Baby Face Modeling/stylegan2/pkl\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JLqXE5bGZnlu2BkbLPD5_ZxoO3Nz-AvF\n",
            "To: /content/drive/My Drive/new/2. Baby Face Modeling/stylegan2/pkl/inception_v3_features.pkl\n",
            "87.3MB [00:01, 66.3MB/s]\n",
            "/content/drive/My Drive/new/2. Baby Face Modeling/stylegan2\n",
            "/content/drive/My Drive/new/2. Baby Face Modeling/stylegan2/results/00001-pretrained\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UlDmJVLLnBD9SnLSMXeiZRO6g-OMQCA_\n",
            "To: /content/drive/My Drive/new/2. Baby Face Modeling/stylegan2/results/00001-pretrained/stylegan2-ffhq-config-f.pkl\n",
            "382MB [00:02, 155MB/s]\n",
            "/content/drive/My Drive/new/2. Baby Face Modeling/stylegan2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN2K68CE27py"
      },
      "source": [
        "##Picking up from a previous session\n",
        "If you already have the StyleGAN2 repo installed in Drive skip the above cell and run the following. This will make sure you have the latest version in case I make updates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8WgjhRFsFJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7094dfa9-5944-4787-f256-1b9e7fac23f8"
      },
      "source": [
        "#USE this if you already have a stylegan2 folder in google drive\n",
        "%cd /content/drive/MyDrive/new/2. Baby Face Modeling/stylegan2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/new/2. Baby Face Modeling/stylegan2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSbEY2pT3TOb"
      },
      "source": [
        "##Make sure Tensorflow 1.15 is set\n",
        "Colab now defaults to Tensorflow 2. Make sure you run this cell to reset it to TF1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMdpKY1XODz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03774f8-4d13-4071-fdd8-7b381379fb6f"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1pIBZGzZxSA"
      },
      "source": [
        "##Converting your dataset\n",
        "StyleGAN requires you to convert your standard jpg or png images into a new format (.tfrecords). \n",
        "\n",
        "I’ve seen some recommendations to run this command every time you restart your Colab machine. I think if you ahve a small-ish dataset (< 2000 images) that’s probably unnecessary.\n",
        "\n",
        "I recommend you upload your dataset to Google Drive and copy its path from the Drive folder in Colab and paste its path in the below cell.\n",
        "\n",
        "After the `create_from_images` argument you need to pass in two paths. The first path is where the .tfrecords files should be output (just edit the last part to have a unique name). The second path is to the directory of your images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixjcx2-cbTDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c88a9c-602c-4ee8-9748-55410d419578"
      },
      "source": [
        "#2nd argument is where to put your tfrecords files\n",
        "#3rd should point at your image dataset\n",
        "!python dataset_tool.py create_from_images ./datasets/babyface ./dataset/babyface"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading images from \"./dataset/babyface\"\n",
            "100% 407/407 [05:24<00:00,  1.25it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAfUNG60aRD1"
      },
      "source": [
        "##Training\n",
        "Note: this will require you to restart your Colab machine every 8–16 hours. You’ve been warned!\n",
        "\n",
        "This library is set up to automatically find your latest .pkl file so it should keep re-training and making progress.\n",
        "\n",
        "##Training Options\n",
        "`--dataset`\n",
        "\n",
        "This should be the name you used in the first path when converting your dataset.\n",
        "\n",
        "`--mirror-augment`\n",
        "\n",
        "Using this option loads some images at random mirrored horizontally (left-to-right). This might help if you have a very small dataset.\n",
        "\n",
        "`--metrics`\n",
        "\n",
        "METRICS DON’T MATTER. It’s art! Use your eyes. Set `--metrics=None` and live your life.\n",
        "\n",
        "If you must use metrics, you have a few options. `fid50k`, the default, uses Frechet Inception Distance score. It’s what was used in StyleGAN1 and what most people know. It’s fine for images of animals and things, but it’s not great. `ppl_wend` is what StyleGAN2 prefers and claims to be more accurate. There are a bunch of other options but I’d recommend you stick with those. Note that both of these take 30–45minutes to run every time it runs so that cuts into your training time in Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QrOVqEHaipA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed60760-82d4-4f3c-afdd-f8777e2739c2"
      },
      "source": [
        "!python run_training.py --num-gpus=1 --data-dir=./datasets --config=config-f --dataset=babyface --mirror-augment=true --metrics=None --network_snapshot_ticks 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local submit - run_dir: results/00002-stylegan2-babyface-1gpu-config-f\n",
            "dnnlib: Running training.training_loop.training_loop() on localhost...\n",
            "Streaming data using training.dataset.TFRecordDataset...\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7942000 @  0x7f8a6791f001 0x7f8a6436b765 0x7f8a643cfbb0 0x7f8a643d1a4f 0x7f8a64468048 0x50a4a5 0x50cc96 0x507be4 0x508ec2 0x594a01 0x549e8f 0x5515c1 0x59fd0e 0x50d256 0x507be4 0x588e5c 0x59fd0e 0x50d256 0x507be4 0x588e5c 0x59fd0e 0x50d256 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x588e5c 0x59fd0e\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f88959bc000 @  0x7f8a6791d1e7 0x7f8a6436b5e1 0x7f8a643cfc78 0x7f8a643cff37 0x7f8a64467f28 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f87949ba000 @  0x7f8a6791d1e7 0x7f8a6436b5e1 0x7f8a643cfc78 0x7f8a643cff37 0x7f8a1fff60c5 0x7f8a1f979902 0x7f8a1f979eb2 0x7f8a1f932c3e 0x50a12f 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x588c8b 0x59fd0e 0x50d256 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x508ec2 0x594a01\n",
            "Dataset shape = [3, 1024, 1024]\n",
            "Dynamic range = [0, 255]\n",
            "Label size    = 0\n",
            "Loading networks from \"results/00001-pretrained/network-snapshot-10000.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Compiling... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Compiling... Loading... Done.\n",
            "\n",
            "G                               Params    OutputShape          WeightShape     \n",
            "---                             ---       ---                  ---             \n",
            "latents_in                      -         (?, 512)             -               \n",
            "labels_in                       -         (?, 0)               -               \n",
            "lod                             -         ()                   -               \n",
            "dlatent_avg                     -         (512,)               -               \n",
            "G_mapping/latents_in            -         (?, 512)             -               \n",
            "G_mapping/labels_in             -         (?, 0)               -               \n",
            "G_mapping/Normalize             -         (?, 512)             -               \n",
            "G_mapping/Dense0                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense1                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense2                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense3                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense4                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense5                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense6                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense7                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Broadcast             -         (?, 18, 512)         -               \n",
            "G_mapping/dlatents_out          -         (?, 18, 512)         -               \n",
            "Truncation/Lerp                 -         (?, 18, 512)         -               \n",
            "G_synthesis/dlatents_in         -         (?, 18, 512)         -               \n",
            "G_synthesis/4x4/Const           8192      (?, 512, 4, 4)       (1, 512, 4, 4)  \n",
            "G_synthesis/4x4/Conv            2622465   (?, 512, 4, 4)       (3, 3, 512, 512)\n",
            "G_synthesis/4x4/ToRGB           264195    (?, 3, 4, 4)         (1, 1, 512, 3)  \n",
            "G_synthesis/8x8/Conv0_up        2622465   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Conv1           2622465   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Upsample        -         (?, 3, 8, 8)         -               \n",
            "G_synthesis/8x8/ToRGB           264195    (?, 3, 8, 8)         (1, 1, 512, 3)  \n",
            "G_synthesis/16x16/Conv0_up      2622465   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Conv1         2622465   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Upsample      -         (?, 3, 16, 16)       -               \n",
            "G_synthesis/16x16/ToRGB         264195    (?, 3, 16, 16)       (1, 1, 512, 3)  \n",
            "G_synthesis/32x32/Conv0_up      2622465   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Conv1         2622465   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Upsample      -         (?, 3, 32, 32)       -               \n",
            "G_synthesis/32x32/ToRGB         264195    (?, 3, 32, 32)       (1, 1, 512, 3)  \n",
            "G_synthesis/64x64/Conv0_up      2622465   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Conv1         2622465   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Upsample      -         (?, 3, 64, 64)       -               \n",
            "G_synthesis/64x64/ToRGB         264195    (?, 3, 64, 64)       (1, 1, 512, 3)  \n",
            "G_synthesis/128x128/Conv0_up    1442561   (?, 256, 128, 128)   (3, 3, 512, 256)\n",
            "G_synthesis/128x128/Conv1       721409    (?, 256, 128, 128)   (3, 3, 256, 256)\n",
            "G_synthesis/128x128/Upsample    -         (?, 3, 128, 128)     -               \n",
            "G_synthesis/128x128/ToRGB       132099    (?, 3, 128, 128)     (1, 1, 256, 3)  \n",
            "G_synthesis/256x256/Conv0_up    426369    (?, 128, 256, 256)   (3, 3, 256, 128)\n",
            "G_synthesis/256x256/Conv1       213249    (?, 128, 256, 256)   (3, 3, 128, 128)\n",
            "G_synthesis/256x256/Upsample    -         (?, 3, 256, 256)     -               \n",
            "G_synthesis/256x256/ToRGB       66051     (?, 3, 256, 256)     (1, 1, 128, 3)  \n",
            "G_synthesis/512x512/Conv0_up    139457    (?, 64, 512, 512)    (3, 3, 128, 64) \n",
            "G_synthesis/512x512/Conv1       69761     (?, 64, 512, 512)    (3, 3, 64, 64)  \n",
            "G_synthesis/512x512/Upsample    -         (?, 3, 512, 512)     -               \n",
            "G_synthesis/512x512/ToRGB       33027     (?, 3, 512, 512)     (1, 1, 64, 3)   \n",
            "G_synthesis/1024x1024/Conv0_up  51297     (?, 32, 1024, 1024)  (3, 3, 64, 32)  \n",
            "G_synthesis/1024x1024/Conv1     25665     (?, 32, 1024, 1024)  (3, 3, 32, 32)  \n",
            "G_synthesis/1024x1024/Upsample  -         (?, 3, 1024, 1024)   -               \n",
            "G_synthesis/1024x1024/ToRGB     16515     (?, 3, 1024, 1024)   (1, 1, 32, 3)   \n",
            "G_synthesis/images_out          -         (?, 3, 1024, 1024)   -               \n",
            "G_synthesis/noise0              -         (1, 1, 4, 4)         -               \n",
            "G_synthesis/noise1              -         (1, 1, 8, 8)         -               \n",
            "G_synthesis/noise2              -         (1, 1, 8, 8)         -               \n",
            "G_synthesis/noise3              -         (1, 1, 16, 16)       -               \n",
            "G_synthesis/noise4              -         (1, 1, 16, 16)       -               \n",
            "G_synthesis/noise5              -         (1, 1, 32, 32)       -               \n",
            "G_synthesis/noise6              -         (1, 1, 32, 32)       -               \n",
            "G_synthesis/noise7              -         (1, 1, 64, 64)       -               \n",
            "G_synthesis/noise8              -         (1, 1, 64, 64)       -               \n",
            "G_synthesis/noise9              -         (1, 1, 128, 128)     -               \n",
            "G_synthesis/noise10             -         (1, 1, 128, 128)     -               \n",
            "G_synthesis/noise11             -         (1, 1, 256, 256)     -               \n",
            "G_synthesis/noise12             -         (1, 1, 256, 256)     -               \n",
            "G_synthesis/noise13             -         (1, 1, 512, 512)     -               \n",
            "G_synthesis/noise14             -         (1, 1, 512, 512)     -               \n",
            "G_synthesis/noise15             -         (1, 1, 1024, 1024)   -               \n",
            "G_synthesis/noise16             -         (1, 1, 1024, 1024)   -               \n",
            "images_out                      -         (?, 3, 1024, 1024)   -               \n",
            "---                             ---       ---                  ---             \n",
            "Total                           30370060                                       \n",
            "\n",
            "\n",
            "D                     Params    OutputShape          WeightShape     \n",
            "---                   ---       ---                  ---             \n",
            "images_in             -         (?, 3, 1024, 1024)   -               \n",
            "labels_in             -         (?, 0)               -               \n",
            "1024x1024/FromRGB     128       (?, 32, 1024, 1024)  (1, 1, 3, 32)   \n",
            "1024x1024/Conv0       9248      (?, 32, 1024, 1024)  (3, 3, 32, 32)  \n",
            "1024x1024/Conv1_down  18496     (?, 64, 512, 512)    (3, 3, 32, 64)  \n",
            "1024x1024/Skip        2048      (?, 64, 512, 512)    (1, 1, 32, 64)  \n",
            "512x512/Conv0         36928     (?, 64, 512, 512)    (3, 3, 64, 64)  \n",
            "512x512/Conv1_down    73856     (?, 128, 256, 256)   (3, 3, 64, 128) \n",
            "512x512/Skip          8192      (?, 128, 256, 256)   (1, 1, 64, 128) \n",
            "256x256/Conv0         147584    (?, 128, 256, 256)   (3, 3, 128, 128)\n",
            "256x256/Conv1_down    295168    (?, 256, 128, 128)   (3, 3, 128, 256)\n",
            "256x256/Skip          32768     (?, 256, 128, 128)   (1, 1, 128, 256)\n",
            "128x128/Conv0         590080    (?, 256, 128, 128)   (3, 3, 256, 256)\n",
            "128x128/Conv1_down    1180160   (?, 512, 64, 64)     (3, 3, 256, 512)\n",
            "128x128/Skip          131072    (?, 512, 64, 64)     (1, 1, 256, 512)\n",
            "64x64/Conv0           2359808   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "64x64/Conv1_down      2359808   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "64x64/Skip            262144    (?, 512, 32, 32)     (1, 1, 512, 512)\n",
            "32x32/Conv0           2359808   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "32x32/Conv1_down      2359808   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "32x32/Skip            262144    (?, 512, 16, 16)     (1, 1, 512, 512)\n",
            "16x16/Conv0           2359808   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "16x16/Conv1_down      2359808   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "16x16/Skip            262144    (?, 512, 8, 8)       (1, 1, 512, 512)\n",
            "8x8/Conv0             2359808   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "8x8/Conv1_down        2359808   (?, 512, 4, 4)       (3, 3, 512, 512)\n",
            "8x8/Skip              262144    (?, 512, 4, 4)       (1, 1, 512, 512)\n",
            "4x4/MinibatchStddev   -         (?, 513, 4, 4)       -               \n",
            "4x4/Conv              2364416   (?, 512, 4, 4)       (3, 3, 513, 512)\n",
            "4x4/Dense0            4194816   (?, 512)             (8192, 512)     \n",
            "Output                513       (?, 1)               (512, 1)        \n",
            "scores_out            -         (?, 1)               -               \n",
            "---                   ---       ---                  ---             \n",
            "Total                 29012513                                       \n",
            "\n",
            "Building TensorFlow graph...\n",
            "Initializing logs...\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 10000.1  lod 0.00  minibatch 32   time 1m 20s       sec/tick 79.9    sec/kimg 624.48  maintenance 0.0    gpumem 13.1\n",
            "tick 1     kimg 10004.2  lod 0.00  minibatch 32   time 30m 19s      sec/tick 1718.7  sec/kimg 419.60  maintenance 20.8   gpumem 13.1\n",
            "tick 2     kimg 10008.3  lod 0.00  minibatch 32   time 59m 00s      sec/tick 1716.3  sec/kimg 419.03  maintenance 4.6    gpumem 13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ7U1ftuj_Dc"
      },
      "source": [
        "Once running, your training files will show up in the results folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9vCDt9LRtXl"
      },
      "source": [
        "#Testing the model (generating images)\n",
        "The following command will generate 55 sample images from the model.\n",
        "\n",
        "##Options\n",
        "`--network`\n",
        "\n",
        "Make sure the `--network` argument points to your .pkl file. (My preferred method is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that into the argument after the `=` sign).\n",
        "\n",
        "`--seeds`\n",
        "\n",
        "This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation.\n",
        "\n",
        "`--truncation-psi`\n",
        "\n",
        "Truncation is a special argument of StyleGAN. Essentially values that are closer to 0 will be more real than numbers further away from 0. I generally recommend a value between `0.5` and `1.0`. `0.5` will give you pretty \"realistic\" results, while `1.0` is likely to give you \"weirder\" results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3MhXEAMOMXH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "537f4e6e-4150-4a73-efe8-9a593bda6de9"
      },
      "source": [
        "!python run_generator.py generate-images --network=/content/ladiesfloralcrop-network-snapshot-010237.pkl --seeds=3875451-3876000 --truncation-psi=0.7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local submit - run_dir: results/00000-generate-images\n",
            "dnnlib: Running run_generator.generate_images() on localhost...\n",
            "Loading networks from \"/content/ladiesfloralcrop-network-snapshot-010237.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Compiling... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Compiling... Loading... Done.\n",
            "Generating image for seed 1 (0/25) ...\n",
            "Generating image for seed 2 (1/25) ...\n",
            "Generating image for seed 3 (2/25) ...\n",
            "Generating image for seed 4 (3/25) ...\n",
            "Generating image for seed 5 (4/25) ...\n",
            "Generating image for seed 6 (5/25) ...\n",
            "Generating image for seed 7 (6/25) ...\n",
            "Generating image for seed 8 (7/25) ...\n",
            "Generating image for seed 9 (8/25) ...\n",
            "Generating image for seed 10 (9/25) ...\n",
            "Generating image for seed 11 (10/25) ...\n",
            "Generating image for seed 12 (11/25) ...\n",
            "Generating image for seed 13 (12/25) ...\n",
            "Generating image for seed 14 (13/25) ...\n",
            "Generating image for seed 15 (14/25) ...\n",
            "Generating image for seed 16 (15/25) ...\n",
            "Generating image for seed 17 (16/25) ...\n",
            "Generating image for seed 18 (17/25) ...\n",
            "Generating image for seed 19 (18/25) ...\n",
            "Generating image for seed 20 (19/25) ...\n",
            "Generating image for seed 21 (20/25) ...\n",
            "Generating image for seed 22 (21/25) ...\n",
            "Generating image for seed 23 (22/25) ...\n",
            "Generating image for seed 24 (23/25) ...\n",
            "Generating image for seed 25 (24/25) ...\n",
            "dnnlib: Finished run_generator.generate_images() in 1m 19s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMiqkA3IReZB"
      },
      "source": [
        "Let’s zip the generated files and download them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp8O01O3PlFx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "e9e84d6d-1523-4283-af1e-11299a014c6b"
      },
      "source": [
        "!zip -r generated-0.7.zip /content/stylegan2/results/00000-generate-images"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/stylegan2/results/00000-generate-images/ (stored 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0025.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0014.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0007.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0018.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0017.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0010.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0015.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0024.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0021.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0004.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/log.txt (deflated 74%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0012.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0009.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0022.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0013.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0011.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0008.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0002.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0001.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/submit_config.txt (deflated 53%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/submit_config.pkl (deflated 43%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0019.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0016.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/run.txt (deflated 34%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0006.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0005.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/_finished.txt (stored 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0023.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0020.png (deflated 0%)\n",
            "  adding: content/stylegan2/results/00000-generate-images/seed0003.png (deflated 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTwJjmCrlfAc"
      },
      "source": [
        "##Interpolation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ2rYIC4TdaJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "889a6ced-d1bc-421f-8d71-a6afedd5310c"
      },
      "source": [
        "!python run_generator.py generate-latent-walk --network=/content/ladiesfloralcrop-network-snapshot-010237.pkl --seeds=3,11,17,25,3 --frames 200 --truncation-psi=0.7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local submit - run_dir: results/00002-generate-latent-walk\n",
            "dnnlib: Running run_generator.generate_latent_walk() on localhost...\n",
            "Loading networks from \"/content/ladiesfloralcrop-network-snapshot-010237.pkl\"...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Loading... Done.\n",
            "Generating image for step 0/204 ...\n",
            "Generating image for step 1/204 ...\n",
            "Generating image for step 2/204 ...\n",
            "Generating image for step 3/204 ...\n",
            "Generating image for step 4/204 ...\n",
            "Generating image for step 5/204 ...\n",
            "Generating image for step 6/204 ...\n",
            "Generating image for step 7/204 ...\n",
            "Generating image for step 8/204 ...\n",
            "Generating image for step 9/204 ...\n",
            "Generating image for step 10/204 ...\n",
            "Generating image for step 11/204 ...\n",
            "Generating image for step 12/204 ...\n",
            "Generating image for step 13/204 ...\n",
            "Generating image for step 14/204 ...\n",
            "Generating image for step 15/204 ...\n",
            "Generating image for step 16/204 ...\n",
            "Generating image for step 17/204 ...\n",
            "Generating image for step 18/204 ...\n",
            "Generating image for step 19/204 ...\n",
            "Generating image for step 20/204 ...\n",
            "Generating image for step 21/204 ...\n",
            "Generating image for step 22/204 ...\n",
            "Generating image for step 23/204 ...\n",
            "Generating image for step 24/204 ...\n",
            "Generating image for step 25/204 ...\n",
            "Generating image for step 26/204 ...\n",
            "Generating image for step 27/204 ...\n",
            "Generating image for step 28/204 ...\n",
            "Generating image for step 29/204 ...\n",
            "Generating image for step 30/204 ...\n",
            "Generating image for step 31/204 ...\n",
            "Generating image for step 32/204 ...\n",
            "Generating image for step 33/204 ...\n",
            "Generating image for step 34/204 ...\n",
            "Generating image for step 35/204 ...\n",
            "Generating image for step 36/204 ...\n",
            "Generating image for step 37/204 ...\n",
            "Generating image for step 38/204 ...\n",
            "Generating image for step 39/204 ...\n",
            "Generating image for step 40/204 ...\n",
            "Generating image for step 41/204 ...\n",
            "Generating image for step 42/204 ...\n",
            "Generating image for step 43/204 ...\n",
            "Generating image for step 44/204 ...\n",
            "Generating image for step 45/204 ...\n",
            "Generating image for step 46/204 ...\n",
            "Generating image for step 47/204 ...\n",
            "Generating image for step 48/204 ...\n",
            "Generating image for step 49/204 ...\n",
            "Generating image for step 50/204 ...\n",
            "Generating image for step 51/204 ...\n",
            "Generating image for step 52/204 ...\n",
            "Generating image for step 53/204 ...\n",
            "Generating image for step 54/204 ...\n",
            "Generating image for step 55/204 ...\n",
            "Generating image for step 56/204 ...\n",
            "Generating image for step 57/204 ...\n",
            "Generating image for step 58/204 ...\n",
            "Generating image for step 59/204 ...\n",
            "Generating image for step 60/204 ...\n",
            "Generating image for step 61/204 ...\n",
            "Generating image for step 62/204 ...\n",
            "Generating image for step 63/204 ...\n",
            "Generating image for step 64/204 ...\n",
            "Generating image for step 65/204 ...\n",
            "Generating image for step 66/204 ...\n",
            "Generating image for step 67/204 ...\n",
            "Generating image for step 68/204 ...\n",
            "Generating image for step 69/204 ...\n",
            "Generating image for step 70/204 ...\n",
            "Generating image for step 71/204 ...\n",
            "Generating image for step 72/204 ...\n",
            "Generating image for step 73/204 ...\n",
            "Generating image for step 74/204 ...\n",
            "Generating image for step 75/204 ...\n",
            "Generating image for step 76/204 ...\n",
            "Generating image for step 77/204 ...\n",
            "Generating image for step 78/204 ...\n",
            "Generating image for step 79/204 ...\n",
            "Generating image for step 80/204 ...\n",
            "Generating image for step 81/204 ...\n",
            "Generating image for step 82/204 ...\n",
            "Generating image for step 83/204 ...\n",
            "Generating image for step 84/204 ...\n",
            "Generating image for step 85/204 ...\n",
            "Generating image for step 86/204 ...\n",
            "Generating image for step 87/204 ...\n",
            "Generating image for step 88/204 ...\n",
            "Generating image for step 89/204 ...\n",
            "Generating image for step 90/204 ...\n",
            "Generating image for step 91/204 ...\n",
            "Generating image for step 92/204 ...\n",
            "Generating image for step 93/204 ...\n",
            "Generating image for step 94/204 ...\n",
            "Generating image for step 95/204 ...\n",
            "Generating image for step 96/204 ...\n",
            "Generating image for step 97/204 ...\n",
            "Generating image for step 98/204 ...\n",
            "Generating image for step 99/204 ...\n",
            "Generating image for step 100/204 ...\n",
            "Generating image for step 101/204 ...\n",
            "Generating image for step 102/204 ...\n",
            "Generating image for step 103/204 ...\n",
            "Generating image for step 104/204 ...\n",
            "Generating image for step 105/204 ...\n",
            "Generating image for step 106/204 ...\n",
            "Generating image for step 107/204 ...\n",
            "Generating image for step 108/204 ...\n",
            "Generating image for step 109/204 ...\n",
            "Generating image for step 110/204 ...\n",
            "Generating image for step 111/204 ...\n",
            "Generating image for step 112/204 ...\n",
            "Generating image for step 113/204 ...\n",
            "Generating image for step 114/204 ...\n",
            "Generating image for step 115/204 ...\n",
            "Generating image for step 116/204 ...\n",
            "Generating image for step 117/204 ...\n",
            "Generating image for step 118/204 ...\n",
            "Generating image for step 119/204 ...\n",
            "Generating image for step 120/204 ...\n",
            "Generating image for step 121/204 ...\n",
            "Generating image for step 122/204 ...\n",
            "Generating image for step 123/204 ...\n",
            "Generating image for step 124/204 ...\n",
            "Generating image for step 125/204 ...\n",
            "Generating image for step 126/204 ...\n",
            "Generating image for step 127/204 ...\n",
            "Generating image for step 128/204 ...\n",
            "Generating image for step 129/204 ...\n",
            "Generating image for step 130/204 ...\n",
            "Generating image for step 131/204 ...\n",
            "Generating image for step 132/204 ...\n",
            "Generating image for step 133/204 ...\n",
            "Generating image for step 134/204 ...\n",
            "Generating image for step 135/204 ...\n",
            "Generating image for step 136/204 ...\n",
            "Generating image for step 137/204 ...\n",
            "Generating image for step 138/204 ...\n",
            "Generating image for step 139/204 ...\n",
            "Generating image for step 140/204 ...\n",
            "Generating image for step 141/204 ...\n",
            "Generating image for step 142/204 ...\n",
            "Generating image for step 143/204 ...\n",
            "Generating image for step 144/204 ...\n",
            "Generating image for step 145/204 ...\n",
            "Generating image for step 146/204 ...\n",
            "Generating image for step 147/204 ...\n",
            "Generating image for step 148/204 ...\n",
            "Generating image for step 149/204 ...\n",
            "Generating image for step 150/204 ...\n",
            "Generating image for step 151/204 ...\n",
            "Generating image for step 152/204 ...\n",
            "Generating image for step 153/204 ...\n",
            "Generating image for step 154/204 ...\n",
            "Generating image for step 155/204 ...\n",
            "Generating image for step 156/204 ...\n",
            "Generating image for step 157/204 ...\n",
            "Generating image for step 158/204 ...\n",
            "Generating image for step 159/204 ...\n",
            "Generating image for step 160/204 ...\n",
            "Generating image for step 161/204 ...\n",
            "Generating image for step 162/204 ...\n",
            "Generating image for step 163/204 ...\n",
            "Generating image for step 164/204 ...\n",
            "Generating image for step 165/204 ...\n",
            "Generating image for step 166/204 ...\n",
            "Generating image for step 167/204 ...\n",
            "Generating image for step 168/204 ...\n",
            "Generating image for step 169/204 ...\n",
            "Generating image for step 170/204 ...\n",
            "Generating image for step 171/204 ...\n",
            "Generating image for step 172/204 ...\n",
            "Generating image for step 173/204 ...\n",
            "Generating image for step 174/204 ...\n",
            "Generating image for step 175/204 ...\n",
            "Generating image for step 176/204 ...\n",
            "Generating image for step 177/204 ...\n",
            "Generating image for step 178/204 ...\n",
            "Generating image for step 179/204 ...\n",
            "Generating image for step 180/204 ...\n",
            "Generating image for step 181/204 ...\n",
            "Generating image for step 182/204 ...\n",
            "Generating image for step 183/204 ...\n",
            "Generating image for step 184/204 ...\n",
            "Generating image for step 185/204 ...\n",
            "Generating image for step 186/204 ...\n",
            "Generating image for step 187/204 ...\n",
            "Generating image for step 188/204 ...\n",
            "Generating image for step 189/204 ...\n",
            "Generating image for step 190/204 ...\n",
            "Generating image for step 191/204 ...\n",
            "Generating image for step 192/204 ...\n",
            "Generating image for step 193/204 ...\n",
            "Generating image for step 194/204 ...\n",
            "Generating image for step 195/204 ...\n",
            "Generating image for step 196/204 ...\n",
            "Generating image for step 197/204 ...\n",
            "Generating image for step 198/204 ...\n",
            "Generating image for step 199/204 ...\n",
            "Generating image for step 200/204 ...\n",
            "Generating image for step 201/204 ...\n",
            "Generating image for step 202/204 ...\n",
            "Generating image for step 203/204 ...\n",
            "dnnlib: Finished run_generator.generate_latent_walk() in 2m 20s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dceBSxTsmW1H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "3e7f602e-8ec4-461d-ab6c-5aee753fff83"
      },
      "source": [
        "#convert to video \n",
        "!ffmpeg -r 24 -i ./results/00001-generate-latent-walk/step%05d.png -vcodec libx264 -pix_fmt yuv420p latent-walk-v2.mp4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.6-0ubuntu0.18.04.1 Copyright (c) 2000-2019 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.3.0-16ubuntu3)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.18.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from './results/00001-generate-latent-walk/step%05d.png':\n",
            "  Duration: 00:00:08.16, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: png, rgb24(pc), 1024x1024, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mprofile High, level 3.2\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=24 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'latent-walk-v2.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1024x1024, q=-1--1, 24 fps, 12288 tbn, 24 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=  204 fps=7.7 q=-1.0 Lsize=   12124kB time=00:00:08.37 bitrate=11859.2kbits/s speed=0.315x    \n",
            "video:12121kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.024653%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mframe I:1     Avg QP:27.07  size:147205\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mframe P:93    Avg QP:27.89  size: 81048\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mframe B:110   Avg QP:30.42  size: 42971\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mconsecutive B-frames: 27.5%  2.0%  0.0% 70.6%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mmb I  I16..4:  8.6% 57.3% 34.2%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mmb P  I16..4:  0.6% 15.3%  8.5%  P16..4: 32.1% 22.5% 15.4%  0.0%  0.0%    skip: 5.6%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mmb B  I16..4:  0.1%  2.7%  2.8%  B16..8: 27.2% 16.2%  8.5%  direct:17.4%  skip:25.2%  L0:29.0% L1:34.5% BI:36.5%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0m8x8 transform intra:59.7% inter:56.9%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mcoded y,uvDC,uvAC intra: 88.8% 83.9% 51.5% inter: 53.3% 28.1% 1.7%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mi16 v,h,dc,p: 43% 22% 21% 14%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 16% 11% 12%  8% 11% 12% 10% 11% 10%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 12% 13%  7% 13% 12% 11%  9%  7%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mi8c dc,h,v,p: 52% 18% 21%  9%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mWeighted P-Frames: Y:19.4% UV:14.0%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mref P L0: 64.4% 26.9%  6.1%  2.0%  0.5%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mref B L0: 95.6%  3.3%  1.1%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mref B L1: 99.1%  0.9%\n",
            "\u001b[1;36m[libx264 @ 0x55a800235e00] \u001b[0mkb/s:11681.39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7A7jRRGmzhU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7cdce8b6-3ee4-459a-cff8-c787bd016269"
      },
      "source": [
        "rm -r /content/drive/My Drive/stylegan2-colab-test/stylegan2/results/00002-stylegan2-birdaus-1gpu-config-f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/drive/My': No such file or directory\n",
            "rm: cannot remove 'Drive/stylegan2-colab-test/stylegan2/results/00002-stylegan2-birdaus-1gpu-config-f': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9UpP_fVdql9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}